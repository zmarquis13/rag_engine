{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a68fe3-9c64-4aff-9ff1-95eb66fac8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Ask a question (or type 'exit' to exit):  what is a pushdown automaton?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No answers found in context library. Searching the web...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Response:** A **pushdown automaton** (PDA) is a type of computational model that extends finite-state machines with an additional memory structure known as a stack. This stack allows the PDA to maintain a form of \"memory\" that can store an unbounded amount of information, enabling it to recognize a broader class of languages, specifically **context-free languages**.\n",
       "\n",
       "Formally, a pushdown automaton can be defined as a 7-tuple:\n",
       "\n",
       "$$\n",
       "M = (Q, \\Sigma, \\Gamma, \\delta, q_0, Z, F)\n",
       "$$\n",
       "\n",
       "where:\n",
       "\n",
       "- $Q$ is a finite set of states,\n",
       "- $\\Sigma$ is a finite input alphabet,\n",
       "- $\\Gamma$ is a finite stack alphabet,\n",
       "- $\\delta$ is a transition function mapping $Q \\times (\\Sigma \\cup \\{\\varepsilon\\}) \\times \\Gamma \\rightarrow Q \\times \\Gamma^*$,\n",
       "- $q_0 \\in Q$ is the start state,\n",
       "- $Z \\in \\Gamma$ is the initial stack symbol,\n",
       "- $F \\subseteq Q$ is the set of accepting states.\n",
       "\n",
       "In operation, a PDA reads input strings from left to right while making state transitions based on the current state, the current input symbol, and the symbol on the top of the stack. It can manipulate the stack by pushing symbols onto it, popping symbols off, or leaving it unchanged. \n",
       "\n",
       "There are two types of PDAs:\n",
       "1. **Deterministic Pushdown Automaton (DPDA)**, which has at most one possible action for each state/input/stack combination.\n",
       "2. **Nondeterministic Pushdown Automaton (NPDA)**, which can have multiple possible actions, making it more powerful as it can recognize all context-free languages, while DPDAs can only recognize deterministic context-free languages.\n",
       "\n",
       "Pushdown automata are essential in the theory of computation and are widely used in designing parsers for programming languages and in various applications involving context-free grammars [source: Wikipedia](https://en.wikipedia.org/wiki/Pushdown_automaton)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Ask a question (or type 'exit' to exit):  What is a context-free grammar?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Response:** A context-free grammar (CFG) is a formal grammar that consists of a set of production rules used to generate strings in a language. It is defined by a tuple $G = (V, \\Sigma, R, S)$, where:\n",
       "\n",
       "- $V$ is a finite set of variables (non-terminal symbols),\n",
       "- $\\Sigma$ is a finite set of terminal symbols (the alphabet of the language),\n",
       "- $R$ is a finite set of production rules, each of the form $A \\rightarrow \\alpha$, where $A \\in V$ and $\\alpha \\in (V \\cup \\Sigma)^*$ (a string of variables and terminals),\n",
       "- $S \\in V$ is the start symbol from which the generation of strings begins.\n",
       "\n",
       "In a context-free grammar, the left-hand side of each production rule consists of a single non-terminal symbol, which allows for the generation of strings without regard to the context in which the non-terminal appears. This property makes context-free grammars particularly useful for defining programming languages and for parsing expressions in computational linguistics. \n",
       "\n",
       "For example, a simple context-free grammar for balanced parentheses could be defined as follows:\n",
       "\n",
       "- Variables: $V = \\{S\\}$\n",
       "- Terminals: $\\Sigma = \\{ (, ) \\}$\n",
       "- Production rules: $R = \\{ S \\rightarrow SS, S \\rightarrow (, S, ), S \\rightarrow \\epsilon \\}$\n",
       "- Start symbol: $S$\n",
       "\n",
       "This grammar generates strings like $()$, $(())$, and $(()())$, which represent balanced parentheses."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.schema import messages_from_dict, messages_to_dict\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "from IPython.display import Markdown\n",
    "from IPython.display import Latex\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from openai import OpenAI\n",
    "import numpy as np\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Brave Search API configuration\n",
    "BRAVE_SEARCH_API_KEY = os.environ.get(\"BRAVE_SEARCH_API_KEY\")\n",
    "BRAVE_SEARCH_ENDPOINT = \"https://api.search.brave.com/res/v1/web/search\"\n",
    "\n",
    "# OpenAI API configuration\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "\n",
    "# 1. Web Search Integration using Brave Search\n",
    "def brave_search(query, count=5):\n",
    "    headers = {\n",
    "        \"Accept\": \"application/json\",\n",
    "        \"X-Subscription-Token\": BRAVE_SEARCH_API_KEY\n",
    "    }\n",
    "    params = {\n",
    "        \"q\": query,\n",
    "        \"count\": count\n",
    "    }\n",
    "    response = requests.get(BRAVE_SEARCH_ENDPOINT, headers=headers, params=params)\n",
    "    return response.json()['web']['results']\n",
    "\n",
    "# 2. Web Scraping and Content Extraction\n",
    "def extract_content(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    return soup.get_text(separator=' ', strip=True)\n",
    "\n",
    "# 3. Text Processing (max length for ada is 8192)\n",
    "def preprocess_text(text, max_length=8192):\n",
    "    # Simple preprocessing: truncate to max_length characters\n",
    "    return text[:max_length]\n",
    "\n",
    "# 4. Embedding Generation using OpenAI\n",
    "def generate_embedding(text):\n",
    "    response = client.embeddings.create(\n",
    "        input=text,\n",
    "        model=\"text-embedding-ada-002\"\n",
    "    )\n",
    "    return response.data[0].embedding\n",
    "\n",
    "# 5. Retrieval\n",
    "def retrieve_relevant_info(query_embedding, doc_embeddings, docs, top_k=5):\n",
    "    similarities = np.dot(doc_embeddings, query_embedding)\n",
    "    top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "    return [docs[i] for i in top_indices]\n",
    "\n",
    "# 6 & 7. Prompt Engineering and Language Model Integration\n",
    "def generate_answer(history, query, relevant_info):\n",
    "    prompt = f\"\"\"Given the following information, please provide a concise and informative answer to the query based on the relevant information.\n",
    "\n",
    "History: {history}\n",
    "\n",
    "Query: {query}\n",
    "\n",
    "Relevant Information:\n",
    "{relevant_info}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that provides accurate and concise information based on the given query, chat history, and relevant information retrieved.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]#,\n",
    "        #max_tokens=\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "# Main RAG function\n",
    "def rag_web_search(history, query):\n",
    "    # Perform web search\n",
    "    search_results = brave_search(query)\n",
    "    \n",
    "    # Extract and process content\n",
    "    docs = []\n",
    "    for result in search_results:\n",
    "        content = extract_content(result['url'])\n",
    "        processed_content = preprocess_text(content)\n",
    "        docs.append(processed_content)\n",
    "    \n",
    "    # Generate embeddings\n",
    "    doc_embeddings = [generate_embedding(doc) for doc in docs]\n",
    "    query_embedding = generate_embedding(query)\n",
    "    \n",
    "    # Retrieve relevant information\n",
    "    relevant_info = retrieve_relevant_info(query_embedding, doc_embeddings, docs)\n",
    "    \n",
    "    # Generate final answer\n",
    "    answer = generate_answer(history, query, \"\\n\".join(relevant_info))\n",
    "    \n",
    "    return answer\n",
    "\n",
    "def markdown_to_text(markdown_content):\n",
    "    # Convert Markdown to HTML\n",
    "    html_content = markdown2.markdown(markdown_content)\n",
    "    \n",
    "    # Parse HTML and extract plain text\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "#create a chromadb (vectorized database for RAG) from a given directory of pdf files\n",
    "def create_db_from_pdf_directory(pdf_directory, db_directory):\n",
    "    # initialize an empty list to store all documents\n",
    "    all_documents = []\n",
    "    \n",
    "    # iterate through all files in the directory\n",
    "    for filename in os.listdir(pdf_directory):\n",
    "        if filename.endswith('.pdf'):\n",
    "            pdf_path = os.path.join(pdf_directory, filename)\n",
    "            #print(f\"Processing {filename}...\")\n",
    "            \n",
    "            # load pdf\n",
    "            loader = PyPDFLoader(pdf_path)\n",
    "            documents = loader.load()\n",
    "            \n",
    "            # add documents to the list\n",
    "            all_documents.extend(documents)\n",
    "    \n",
    "    # split text into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    texts = text_splitter.split_documents(all_documents)\n",
    "    \n",
    "    # vector embeddings instance\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    \n",
    "    # create chromadb\n",
    "    db = Chroma.from_documents(texts, embeddings, persist_directory=db_directory)\n",
    "    \n",
    "    print(f\"Database created successfully at {db_directory}\")\n",
    "    return db\n",
    "\n",
    "\n",
    "#specify source and database and create database with it\n",
    "pdf_directory = \"pdfsnew_\"\n",
    "\n",
    "db_directory = \"chromadb\"\n",
    "\n",
    "if not os.path.exists(db_directory):\n",
    "    #print(\"Creating new Chroma database...\")\n",
    "    db = create_db_from_pdf_directory(pdf_directory, db_directory)\n",
    "else:\n",
    "    #print(\"Loading existing Chroma database...\")\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    db = Chroma(embedding_function=embeddings)\n",
    "    #print(\"Database loaded\")\n",
    "\n",
    "\n",
    "def query_db(db, query):\n",
    "    # Create a retriever from the database\n",
    "    retriever = db.as_retriever()\n",
    "    \n",
    "    # Create a ChatOpenAI model\n",
    "    chat = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "    # Add memory to keep track of conversation history\n",
    "    memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "    # Create a Conversational Retrieval Chain with memory\n",
    "    qa_chain = ConversationalRetrievalChain.from_llm(chat, retriever=retriever, memory=memory)\n",
    "    \n",
    "    # Run the query \n",
    "    result = qa_chain({\"memory\": memory, \"question\": query})\n",
    "    \n",
    "    return result['answer']\n",
    "\n",
    "def digest_new_pdfs():\n",
    "    new_pdf_dir = 'new_pdfs'\n",
    "    used_pdf_dir = 'used_pdfs'\n",
    "\n",
    "    added_documents = []\n",
    "\n",
    "    if os.listdir(new_pdf_dir):\n",
    "\n",
    "        print('processing new content')\n",
    "        for file in os.listdir(new_pdf_dir):\n",
    "            if file.endswith(\".pdf\"):\n",
    "                # load pdf\n",
    "                pdf_path = os.path.join(new_pdf_dir, file)\n",
    "                loader = PyPDFLoader(pdf_path)\n",
    "                documents = loader.load()\n",
    "                \n",
    "                # add documents to the list\n",
    "                added_documents.extend(documents)\n",
    "                \n",
    "                source_file = os.path.join(new_pdf_dir, file)\n",
    "                destination_file = os.path.join(used_pdf_dir, file)\n",
    "                shutil.move(source_file, destination_file)\n",
    "    \n",
    "        # split text into chunks\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "        texts = text_splitter.split_documents(added_documents)\n",
    "        \n",
    "        # vector embeddings instance\n",
    "        embeddings = OpenAIEmbeddings()\n",
    "    \n",
    "        # add the embeddings to the existing ChromaDB instance\n",
    "        db.add_documents(documents=texts, embeddings=embeddings)\n",
    "\n",
    "        print('processing complete')\n",
    "\n",
    "def encode_latex(text):\n",
    "    \n",
    "    # Pattern for inline LaTeX: \\(...\\) or \\[...\\]\n",
    "    inline_pattern = r'\\\\[\\(\\[](.+?)\\\\[\\)\\]]'\n",
    "    # Pattern for display LaTeX: \\begin{equation}...\\end{equation} or \\begin{align}...\\end{align}\n",
    "    display_pattern = r'\\\\begin\\{(equation|align)\\}(.+?)\\\\end\\{\\1\\}'\n",
    "    \n",
    "    # Replace inline LaTeX\n",
    "    text = re.sub(inline_pattern, r'$\\1$', text)\n",
    "    \n",
    "    # Replace display LaTeX\n",
    "    text = re.sub(display_pattern, r'$$\\2$$', text, flags=re.DOTALL)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# First digest new pdfs\n",
    "digest_new_pdfs()\n",
    "\n",
    "# Create a retriever from the database\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "# Create a ChatOpenAI model\n",
    "chat = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# Add memory to keep track of conversation history\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# Create a Conversational Retrieval Chain with memory\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(chat, retriever=retriever, memory=memory)\n",
    "\n",
    "history = ''\n",
    "\n",
    "# Interactive loop\n",
    "while True:\n",
    "    query = input(\"\\nAsk a question (or type 'exit' to exit): \")\n",
    "    if query.lower() == 'exit':\n",
    "        break\n",
    "    elif query.lower() == 'add':\n",
    "        # Add new PDFs to ChromaDB\n",
    "        new_pdf_directory = input(\"Enter the path to the directory with new PDFs: \")\n",
    "        # Function to add new PDFs would go here\n",
    "        # add_new_pdfs_to_chromadb(new_pdf_directory, db)\n",
    "        print(\"New PDFs added to ChromaDB.\")\n",
    "    else:\n",
    "        #response = query_db(db, query)\n",
    "        \n",
    "        # Run the query \n",
    "        modified_query = query + \" enclose any latex variables, symbols, or equations in $ and use textual evidence where helpful\"\n",
    "        result = qa_chain({\"question\": modified_query})\n",
    "        answer = result['answer']\n",
    "\n",
    "        # and enclose any backslashed latex expressions or equations in $\n",
    "        \n",
    "        if (answer == \"I don't know.\" or answer.startswith(\"The provided context does not contain\")):\n",
    "            print('No answers found in context library. Searching the web...')\n",
    "            answer = rag_web_search(history, query + \" enclose any latex variables, symbols, or equations in $ and cite specific source links used \")\n",
    "            #answer = Latex(answer)\n",
    "        \n",
    "        history += f\"query: {query}\\n answer: {answer}\\n\"\n",
    "        display(Markdown(\"**Response:** \" + encode_latex(answer)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2226801a-e949-4d35-a3ce-965a27177a9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
