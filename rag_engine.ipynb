{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "69a68fe3-9c64-4aff-9ff1-95eb66fac8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Ask a question (or type 'exit' to exit):  Give me the outline for a 12-week introductory course in machine learning based on the most important topics within the field\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Response:** Here is a suggested outline for a 12-week introductory course in machine learning, based on the important topics within the field as indicated in the provided context:\n",
       "\n",
       "### Week 1: Introduction to Machine Learning\n",
       "- Overview of machine learning and its applications\n",
       "- Types of machine learning: supervised, unsupervised, and reinforcement learning\n",
       "- Importance of understanding simple methods before complex ones\n",
       "\n",
       "### Week 2: Supervised Learning Overview\n",
       "- Introduction to supervised learning problems\n",
       "- Variable types and terminology\n",
       "- Basic concepts of prediction\n",
       "\n",
       "### Week 3: Linear Regression\n",
       "- Introduction to linear models\n",
       "- Least squares method for regression\n",
       "- Equation: $y = \\beta_0 + \\beta_1 x + \\epsilon$\n",
       "\n",
       "### Week 4: Classification Techniques\n",
       "- Overview of classification problems\n",
       "- Introduction to logistic regression\n",
       "- Decision boundaries and the logistic function\n",
       "\n",
       "### Week 5: Nearest Neighbor Methods\n",
       "- Understanding nearest-neighbor algorithms\n",
       "- Comparison with linear regression\n",
       "- Distance metrics and their importance\n",
       "\n",
       "### Week 6: Model Assessment and Selection\n",
       "- Concepts of bias and variance\n",
       "- Overfitting and underfitting\n",
       "- Techniques for model selection (cross-validation)\n",
       "\n",
       "### Week 7: Regularization Techniques\n",
       "- Introduction to regularization methods\n",
       "- Lasso ($L_1$) and Ridge ($L_2$) regression\n",
       "- Importance of regularization in high-dimensional data\n",
       "\n",
       "### Week 8: Non-Linear Methods\n",
       "- Introduction to splines and wavelets\n",
       "- Kernel methods and their applications\n",
       "- Local regression techniques\n",
       "\n",
       "### Week 9: Decision Trees and Ensemble Methods\n",
       "- Overview of decision trees\n",
       "- Introduction to ensemble methods: bagging and boosting\n",
       "- Importance of ensemble methods in improving model performance\n",
       "\n",
       "### Week 10: Support Vector Machines\n",
       "- Introduction to support vector machines (SVM)\n",
       "- Concept of margin and hyperplanes\n",
       "- Equation: $f(x) = w^T x + b$\n",
       "\n",
       "### Week 11: Neural Networks\n",
       "- Basics of neural networks and their architecture\n",
       "- Activation functions and backpropagation\n",
       "- Introduction to deep learning concepts\n",
       "\n",
       "### Week 12: Current Trends and Future Directions\n",
       "- Overview of recent advancements in machine learning\n",
       "- Discussion on ethical considerations in machine learning\n",
       "- Future directions and emerging topics in the field\n",
       "\n",
       "This outline incorporates foundational concepts and techniques in machine learning, emphasizing the importance of understanding simpler methods before progressing to more complex ones, as highlighted in the provided context."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Ask a question (or type 'exit' to exit):  What's currently the most heavily invested-in application of machine learning?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No answers found in context library. Searching the web: \n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Response:** As of now, the most heavily invested-in application of machine learning is in the field of **healthcare**, specifically in areas like drug discovery, diagnostics, and personalized medicine. According to a report by Frost & Sullivan, the global healthcare AI market is projected to reach **$6.6 billion** by 2021, significantly increasing investments across various sectors, especially in predictive analytics and medical imaging.\n",
       "\n",
       "Furthermore, companies like Google and IBM have heavily invested in healthcare applications, with projects focusing on machine learning algorithms for analyzing medical data and predicting patient outcomes.\n",
       "\n",
       "For detailed statistics and analyses, you can refer to sources such as:\n",
       "- Frost & Sullivan: [Healthcare AI Market Report](https://www.frost.com)\n",
       "- McKinsey & Company: [How AI is Transforming Healthcare](https://www.mckinsey.com)\n",
       "- Deloitte Insights: [AI in Healthcare](https://www2.deloitte.com/global/en/pages/life-sciences-and-healthcare/articles/ai-in-healthcare.html)\n",
       "\n",
       "These references provide insights into the scale of investment and the transformative potential of machine learning in the healthcare sector."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Ask a question (or type 'exit' to exit):  exit\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.schema import messages_from_dict, messages_to_dict\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "from IPython.display import Markdown\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from openai import OpenAI\n",
    "import numpy as np\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Brave Search API configuration\n",
    "BRAVE_SEARCH_API_KEY = os.environ.get(\"BRAVE_SEARCH_API_KEY\")\n",
    "BRAVE_SEARCH_ENDPOINT = \"https://api.search.brave.com/res/v1/web/search\"\n",
    "\n",
    "# OpenAI API configuration\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "\n",
    "# 1. Web Search Integration using Brave Search\n",
    "def brave_search(query, count=5):\n",
    "    headers = {\n",
    "        \"Accept\": \"application/json\",\n",
    "        \"X-Subscription-Token\": BRAVE_SEARCH_API_KEY\n",
    "    }\n",
    "    params = {\n",
    "        \"q\": query,\n",
    "        \"count\": count\n",
    "    }\n",
    "    response = requests.get(BRAVE_SEARCH_ENDPOINT, headers=headers, params=params)\n",
    "    return response.json()['web']['results']\n",
    "\n",
    "# 2. Web Scraping and Content Extraction\n",
    "def extract_content(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    return soup.get_text(separator=' ', strip=True)\n",
    "\n",
    "# 3. Text Processing\n",
    "def preprocess_text(text, max_length=10000):\n",
    "    # Simple preprocessing: truncate to max_length characters\n",
    "    return text[:max_length]\n",
    "\n",
    "# 4. Embedding Generation using OpenAI\n",
    "def generate_embedding(text):\n",
    "    response = client.embeddings.create(\n",
    "        input=text,\n",
    "        model=\"text-embedding-ada-002\"\n",
    "    )\n",
    "    return response.data[0].embedding\n",
    "\n",
    "# 5. Retrieval\n",
    "def retrieve_relevant_info(query_embedding, doc_embeddings, docs, top_k=3):\n",
    "    similarities = np.dot(doc_embeddings, query_embedding)\n",
    "    top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "    return [docs[i] for i in top_indices]\n",
    "\n",
    "# 6 & 7. Prompt Engineering and Language Model Integration\n",
    "def generate_answer(history, query, relevant_info):\n",
    "    prompt = f\"\"\"Given the following information, please provide a concise and informative answer to the query.\n",
    "\n",
    "History: {history}\n",
    "\n",
    "Query: {query}\n",
    "\n",
    "Relevant Information:\n",
    "{relevant_info}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that provides accurate and concise information based on the given query, chat history, and relevant information retrieved.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]#,\n",
    "        #max_tokens=\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "# Main RAG function\n",
    "def rag_web_search(history, query):\n",
    "    # Perform web search\n",
    "    search_results = brave_search(query)\n",
    "    \n",
    "    # Extract and process content\n",
    "    docs = []\n",
    "    for result in search_results:\n",
    "        content = extract_content(result['url'])\n",
    "        processed_content = preprocess_text(content)\n",
    "        docs.append(processed_content)\n",
    "    \n",
    "    # Generate embeddings\n",
    "    doc_embeddings = [generate_embedding(doc) for doc in docs]\n",
    "    query_embedding = generate_embedding(query)\n",
    "    \n",
    "    # Retrieve relevant information\n",
    "    relevant_info = retrieve_relevant_info(query_embedding, doc_embeddings, docs)\n",
    "    \n",
    "    # Generate final answer\n",
    "    answer = generate_answer(history, query, \"\\n\".join(relevant_info))\n",
    "    \n",
    "    return answer\n",
    "\n",
    "def markdown_to_text(markdown_content):\n",
    "    # Convert Markdown to HTML\n",
    "    html_content = markdown2.markdown(markdown_content)\n",
    "    \n",
    "    # Parse HTML and extract plain text\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "#create a chromadb (vectorized database for RAG) from a given directory of pdf files\n",
    "def create_db_from_pdf_directory(pdf_directory, db_directory):\n",
    "    # initialize an empty list to store all documents\n",
    "    all_documents = []\n",
    "    \n",
    "    # iterate through all files in the directory\n",
    "    for filename in os.listdir(pdf_directory):\n",
    "        if filename.endswith('.pdf'):\n",
    "            pdf_path = os.path.join(pdf_directory, filename)\n",
    "            #print(f\"Processing {filename}...\")\n",
    "            \n",
    "            # load pdf\n",
    "            loader = PyPDFLoader(pdf_path)\n",
    "            documents = loader.load()\n",
    "            \n",
    "            # add documents to the list\n",
    "            all_documents.extend(documents)\n",
    "    \n",
    "    # split text into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    texts = text_splitter.split_documents(all_documents)\n",
    "    \n",
    "    # vector embeddings instance\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    \n",
    "    # create chromadb\n",
    "    db = Chroma.from_documents(texts, embeddings, persist_directory=db_directory)\n",
    "    \n",
    "    print(f\"Database created successfully at {db_directory}\")\n",
    "    return db\n",
    "\n",
    "\n",
    "#specify source and database and create database with it\n",
    "pdf_directory = \"pdfsnew_\"\n",
    "\n",
    "db_directory = \"chromadb\"\n",
    "\n",
    "if not os.path.exists(db_directory):\n",
    "    #print(\"Creating new Chroma database...\")\n",
    "    db = create_db_from_pdf_directory(pdf_directory, db_directory)\n",
    "else:\n",
    "    #print(\"Loading existing Chroma database...\")\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    db = Chroma(embedding_function=embeddings)\n",
    "    #print(\"Database loaded\")\n",
    "\n",
    "\n",
    "def query_db(db, query):\n",
    "    # Create a retriever from the database\n",
    "    retriever = db.as_retriever()\n",
    "    \n",
    "    # Create a ChatOpenAI model\n",
    "    chat = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "    # Add memory to keep track of conversation history\n",
    "    memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "    # Create a Conversational Retrieval Chain with memory\n",
    "    qa_chain = ConversationalRetrievalChain.from_llm(chat, retriever=retriever, memory=memory)\n",
    "    \n",
    "    # Run the query \n",
    "    result = qa_chain({\"memory\": memory, \"question\": query})\n",
    "    \n",
    "    return result['answer']\n",
    "\n",
    "def digest_new_pdfs():\n",
    "    new_pdf_dir = 'new_pdfs'\n",
    "    used_pdf_dir = 'used_pdfs'\n",
    "\n",
    "    added_documents = []\n",
    "\n",
    "    if os.listdir(new_pdf_dir):\n",
    "\n",
    "        print('processing new content')\n",
    "        for file in os.listdir(new_pdf_dir):\n",
    "            if file.endswith(\".pdf\"):\n",
    "                # load pdf\n",
    "                pdf_path = os.path.join(new_pdf_dir, file)\n",
    "                loader = PyPDFLoader(pdf_path)\n",
    "                documents = loader.load()\n",
    "                \n",
    "                # add documents to the list\n",
    "                added_documents.extend(documents)\n",
    "                \n",
    "                source_file = os.path.join(new_pdf_dir, file)\n",
    "                destination_file = os.path.join(used_pdf_dir, file)\n",
    "                shutil.move(source_file, destination_file)\n",
    "    \n",
    "        # split text into chunks\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "        texts = text_splitter.split_documents(added_documents)\n",
    "        \n",
    "        # vector embeddings instance\n",
    "        embeddings = OpenAIEmbeddings()\n",
    "    \n",
    "        # add the embeddings to the existing ChromaDB instance\n",
    "        db.add_documents(documents=texts, embeddings=embeddings)\n",
    "\n",
    "        print('processing complete')\n",
    "\n",
    "# First digest new pdfs\n",
    "digest_new_pdfs()\n",
    "\n",
    "# Create a retriever from the database\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "# Create a ChatOpenAI model\n",
    "chat = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# Add memory to keep track of conversation history\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# Create a Conversational Retrieval Chain with memory\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(chat, retriever=retriever, memory=memory)\n",
    "\n",
    "history = ''\n",
    "\n",
    "# Interactive loop\n",
    "while True:\n",
    "    query = input(\"\\nAsk a question (or type 'exit' to exit): \")\n",
    "    if query.lower() == 'exit':\n",
    "        break\n",
    "    elif query.lower() == 'add':\n",
    "        # Add new PDFs to ChromaDB\n",
    "        new_pdf_directory = input(\"Enter the path to the directory with new PDFs: \")\n",
    "        # Function to add new PDFs would go here\n",
    "        # add_new_pdfs_to_chromadb(new_pdf_directory, db)\n",
    "        print(\"New PDFs added to ChromaDB.\")\n",
    "    else:\n",
    "        #response = query_db(db, query)\n",
    "        \n",
    "        # Run the query \n",
    "        modified_query = query + \" use textual evidence where beneficial and put dollar signs on each side of any equations used\"\n",
    "        result = qa_chain({\"question\": modified_query})\n",
    "        answer = result['answer']\n",
    "        \n",
    "        if (answer == \"I don't know.\"):\n",
    "            print('No answers found in context library. Searching the web...')\n",
    "            answer = rag_web_search(history, query + \" cite specific source links used and put dollar signs on each side of any latex expressions or equations used\")\n",
    "            \n",
    "        history += f\"query: {query}\\n answer: {answer}\\n\"\n",
    "        display(Markdown(\"**Response:** \" + answer))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
