{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a68fe3-9c64-4aff-9ff1-95eb66fac8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Ask a question (or type 'exit' to exit):  What is the pumping lemma for regular languages?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Response:** The pumping lemma for regular languages is a property that all regular languages must satisfy. It is used to prove that certain languages are not regular. The lemma states that for any regular language $L$, there exists a constant $p$ (the pumping length) such that any string $s$ in $L$ with a length of at least $p$ can be divided into three parts, $s = xyz$, satisfying the following conditions:\n",
       "\n",
       "1. The length of the string $y$ must be greater than 0: $|y| > 0$.\n",
       "2. The length of the combined string $xy$ must be at most $p$: $|xy| \\leq p$.\n",
       "3. For all integers $i \\geq 0$, the string $xy^iz$ must also be in $L$.\n",
       "\n",
       "In summary, the pumping lemma asserts that if a language is regular, then sufficiently long strings in that language can be \"pumped\" (i.e., have a substring repeated) and still remain within the language. This property can be used to show that certain languages do not meet these criteria, thereby proving they are not regular."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Ask a question (or type 'exit' to exit):  What is the pumping lemma for context-free languages?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No answers found in context library. Searching the web...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Response:** The pumping lemma for context-free languages (CFLs) states that if $L$ is a context-free language, then there exists a number $p \\geq 1$ (the pumping length) such that any string $s$ in $L$ with length at least $p$ can be decomposed into five substrings: $s = uvwxy$, satisfying the following conditions:\n",
       "\n",
       "1. $|vy| > 0$: The length of the concatenation of substrings $v$ and $y$ must be greater than zero.\n",
       "2. $|vwx| \\leq p$: The length of the concatenation of substrings $v$, $w$, and $x$ must be at most $p$.\n",
       "3. For all integers $i \\geq 0$, the string $uv^iwx^iy$ must also belong to $L$: Repeating the substrings $v$ and $x$ any number of times should yield a string in the language.\n",
       "\n",
       "This lemma is primarily used to prove that certain languages are not context-free. By showing that no valid decomposition exists for sufficiently long strings in a language, one can infer that the language does not satisfy the conditions of the pumping lemma, thus proving it is not context-free.\n",
       "\n",
       "The formal statement can be expressed as:\n",
       "$$ (\\forall L \\subseteq \\Sigma^*) \\left( \\text{context free}(L) \\Rightarrow \\left( \\exists p \\geq 1 \\left( \\forall s \\in L \\left( |s| \\geq p \\Rightarrow \\exists u, v, w, x, y \\in \\Sigma^* (s = uvwxy \\land |vy| \\geq 1 \\land |vwx| \\leq p \\land \\forall n \\geq 0 (uv^n wx^n y \\in L)) \\right) \\right) \\right) \\right)$$\n",
       "\n",
       "For further reading, you can refer to this [Wikipedia article on the pumping lemma for context-free languages](https://en.wikipedia.org/wiki/Pumping_lemma_for_context-free_languages)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.schema import messages_from_dict, messages_to_dict\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "from IPython.display import Markdown\n",
    "from IPython.display import Latex\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from openai import OpenAI\n",
    "import numpy as np\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Brave Search API configuration\n",
    "BRAVE_SEARCH_API_KEY = os.environ.get(\"BRAVE_SEARCH_API_KEY\")\n",
    "BRAVE_SEARCH_ENDPOINT = \"https://api.search.brave.com/res/v1/web/search\"\n",
    "\n",
    "# OpenAI API configuration\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "\n",
    "# 1. Web Search Integration using Brave Search\n",
    "def brave_search(query, count=5):\n",
    "    headers = {\n",
    "        \"Accept\": \"application/json\",\n",
    "        \"X-Subscription-Token\": BRAVE_SEARCH_API_KEY\n",
    "    }\n",
    "    params = {\n",
    "        \"q\": query,\n",
    "        \"count\": count\n",
    "    }\n",
    "    response = requests.get(BRAVE_SEARCH_ENDPOINT, headers=headers, params=params)\n",
    "    return response.json()['web']['results']\n",
    "\n",
    "# 2. Web Scraping and Content Extraction\n",
    "def extract_content(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    return soup.get_text(separator=' ', strip=True)\n",
    "\n",
    "# 3. Text Processing (max length for ada is 8192)\n",
    "def preprocess_text(text, max_length=8192):\n",
    "    # Simple preprocessing: truncate to max_length characters\n",
    "    return text[:max_length]\n",
    "\n",
    "# 4. Embedding Generation using OpenAI\n",
    "def generate_embedding(text):\n",
    "    response = client.embeddings.create(\n",
    "        input=text,\n",
    "        model=\"text-embedding-ada-002\"\n",
    "    )\n",
    "    return response.data[0].embedding\n",
    "\n",
    "# 5. Retrieval\n",
    "def retrieve_relevant_info(query_embedding, doc_embeddings, docs, top_k=5):\n",
    "    similarities = np.dot(doc_embeddings, query_embedding)\n",
    "    top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "    return [docs[i] for i in top_indices]\n",
    "\n",
    "# 6 & 7. Prompt Engineering and Language Model Integration\n",
    "def generate_answer(history, query, relevant_info):\n",
    "    prompt = f\"\"\"Given the following information, please provide a concise and informative answer to the query based on the relevant information.\n",
    "\n",
    "History: {history}\n",
    "\n",
    "Query: {query}\n",
    "\n",
    "Relevant Information:\n",
    "{relevant_info}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that provides accurate and concise information based on the given query, chat history, and relevant information retrieved.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]#,\n",
    "        #max_tokens=\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "# Main RAG function\n",
    "def rag_web_search(history, query):\n",
    "    # Perform web search\n",
    "    search_results = brave_search(query)\n",
    "    \n",
    "    # Extract and process content\n",
    "    docs = []\n",
    "    for result in search_results:\n",
    "        content = extract_content(result['url'])\n",
    "        processed_content = preprocess_text(content)\n",
    "        docs.append(processed_content)\n",
    "    \n",
    "    # Generate embeddings\n",
    "    doc_embeddings = [generate_embedding(doc) for doc in docs]\n",
    "    query_embedding = generate_embedding(query)\n",
    "    \n",
    "    # Retrieve relevant information\n",
    "    relevant_info = retrieve_relevant_info(query_embedding, doc_embeddings, docs)\n",
    "    \n",
    "    # Generate final answer\n",
    "    answer = generate_answer(history, query, \"\\n\".join(relevant_info))\n",
    "    \n",
    "    return answer\n",
    "\n",
    "def markdown_to_text(markdown_content):\n",
    "    # Convert Markdown to HTML\n",
    "    html_content = markdown2.markdown(markdown_content)\n",
    "    \n",
    "    # Parse HTML and extract plain text\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "#create a chromadb (vectorized database for RAG) from a given directory of pdf files\n",
    "def create_db_from_pdf_directory(pdf_directory, db_directory):\n",
    "    # initialize an empty list to store all documents\n",
    "    all_documents = []\n",
    "    \n",
    "    # iterate through all files in the directory\n",
    "    for filename in os.listdir(pdf_directory):\n",
    "        if filename.endswith('.pdf'):\n",
    "            pdf_path = os.path.join(pdf_directory, filename)\n",
    "            #print(f\"Processing {filename}...\")\n",
    "            \n",
    "            # load pdf\n",
    "            loader = PyPDFLoader(pdf_path)\n",
    "            documents = loader.load()\n",
    "            \n",
    "            # add documents to the list\n",
    "            all_documents.extend(documents)\n",
    "    \n",
    "    # split text into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    texts = text_splitter.split_documents(all_documents)\n",
    "    \n",
    "    # vector embeddings instance\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    \n",
    "    # create chromadb\n",
    "    db = Chroma.from_documents(texts, embeddings, persist_directory=db_directory)\n",
    "    \n",
    "    print(f\"Database created successfully at {db_directory}\")\n",
    "    return db\n",
    "\n",
    "\n",
    "#specify source and database and create database with it\n",
    "pdf_directory = \"new_pdfs\"\n",
    "\n",
    "db_directory = \"chromadb\"\n",
    "\n",
    "if not os.path.exists(db_directory):\n",
    "    #print(\"Creating new Chroma database...\")\n",
    "    try:\n",
    "        db = create_db_from_pdf_directory(pdf_directory, db_directory)\n",
    "    except ValueError:\n",
    "        print(\"Please add content before chatting. Put one or more pdfs in the new_pdfs dir and run this cell again.\")\n",
    "        sys.exit(1)\n",
    "else:\n",
    "    #print(\"Loading existing Chroma database...\")\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    db = Chroma(embedding_function=embeddings)\n",
    "    #print(\"Database loaded\")\n",
    "\n",
    "\n",
    "def query_db(db, query):\n",
    "    # Create a retriever from the database\n",
    "    retriever = db.as_retriever()\n",
    "    \n",
    "    # Create a ChatOpenAI model\n",
    "    chat = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "    # Add memory to keep track of conversation history\n",
    "    memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "    # Create a Conversational Retrieval Chain with memory\n",
    "    qa_chain = ConversationalRetrievalChain.from_llm(chat, retriever=retriever, memory=memory)\n",
    "    \n",
    "    # Run the query \n",
    "    result = qa_chain({\"memory\": memory, \"question\": query})\n",
    "    \n",
    "    return result['answer']\n",
    "\n",
    "def digest_new_pdfs():\n",
    "    new_pdf_dir = 'new_pdfs'\n",
    "    used_pdf_dir = 'used_pdfs'\n",
    "\n",
    "    added_documents = []\n",
    "\n",
    "    if os.listdir(new_pdf_dir):\n",
    "\n",
    "        pdf_found = False\n",
    "\n",
    "        for file in os.listdir(new_pdf_dir):\n",
    "            if file.endswith(\".pdf\"):\n",
    "                if not pdf_found:\n",
    "                    pdf_found = true\n",
    "                    print('processing new content')\n",
    "                \n",
    "                \n",
    "                # load pdf\n",
    "                pdf_path = os.path.join(new_pdf_dir, file)\n",
    "                loader = PyPDFLoader(pdf_path)\n",
    "                documents = loader.load()\n",
    "                \n",
    "                # add documents to the list\n",
    "                added_documents.extend(documents)\n",
    "                \n",
    "                source_file = os.path.join(new_pdf_dir, file)\n",
    "                destination_file = os.path.join(used_pdf_dir, file)\n",
    "                shutil.move(source_file, destination_file)\n",
    "\n",
    "        if pdf_found:\n",
    "            # split text into chunks\n",
    "            text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "            texts = text_splitter.split_documents(added_documents)\n",
    "            \n",
    "            # vector embeddings instance\n",
    "            embeddings = OpenAIEmbeddings()\n",
    "        \n",
    "            # add the embeddings to the existing ChromaDB instance\n",
    "            db.add_documents(documents=texts, embeddings=embeddings)\n",
    "    \n",
    "            print('processing complete')\n",
    "\n",
    "def encode_latex(text):\n",
    "    \n",
    "    # Pattern for inline LaTeX: \\(...\\) or \\[...\\]\n",
    "    inline_pattern = r'\\\\[\\(\\[](.+?)\\\\[\\)\\]]'\n",
    "    # Pattern for display LaTeX: \\begin{equation}...\\end{equation} or \\begin{align}...\\end{align}\n",
    "    display_pattern = r'\\\\begin\\{(equation|align)\\}(.+?)\\\\end\\{\\1\\}'\n",
    "    \n",
    "    # Replace inline LaTeX\n",
    "    text = re.sub(inline_pattern, r'$\\1$', text)\n",
    "    \n",
    "    # Replace display LaTeX\n",
    "    text = re.sub(display_pattern, r'$$\\2$$', text, flags=re.DOTALL)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# First digest new pdfs\n",
    "digest_new_pdfs()\n",
    "\n",
    "# Create a retriever from the database\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "# Create a ChatOpenAI model\n",
    "chat = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# Add memory to keep track of conversation history\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# Create a Conversational Retrieval Chain with memory\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(chat, retriever=retriever, memory=memory)\n",
    "\n",
    "history = ''\n",
    "\n",
    "# Interactive loop\n",
    "while True:\n",
    "    query = input(\"\\nAsk a question (or type 'exit' to exit): \")\n",
    "    if query.lower() == 'exit':\n",
    "        break\n",
    "    else:\n",
    "        # Modify and run the query \n",
    "        modified_query = query + \" enclose any latex expressions, symbols, or equations in $ and use textual evidence where helpful\"\n",
    "        result = qa_chain({\"question\": modified_query})\n",
    "        answer = result['answer']\n",
    "\n",
    "        if (answer == \"I don't know.\" or answer.startswith(\"The provided context does not contain\")):\n",
    "            print('No answers found in context library. Searching the web...')\n",
    "            answer = rag_web_search(history, query + \" enclose any latex explressions, symbols, or equations in $ and cite specific source links used \")\n",
    "        \n",
    "        history += f\"query: {query}\\n answer: {answer}\\n\"\n",
    "        display(Markdown('\\n**Response:** ' + encode_latex(answer) + '\\n'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2226801a-e949-4d35-a3ce-965a27177a9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
