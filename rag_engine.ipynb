{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a68fe3-9c64-4aff-9ff1-95eb66fac8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Ask a question (or type 'exit' to exit):  What's the benefit of using a neural network over regression?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "**Response:** The benefit of using a neural network over traditional regression methods lies in its ability to model complex, nonlinear relationships between inputs and outputs. Neural networks, particularly those with hidden layers, can capture intricate patterns in the data that linear regression may miss.\n",
       "\n",
       "1. **Nonlinearity**: Neural networks can take nonlinear functions of linear combinations of the inputs, which allows them to model complex relationships. As stated in the text, \"Both projection pursuit regression and neural networks take nonlinear functions of linear combinations ('derived features') of the inputs.\" This capability makes neural networks particularly effective in scenarios where the relationship between predictors and the response variable is not linear.\n",
       "\n",
       "2. **Flexibility**: Neural networks can handle multiple quantitative responses and can be adapted for both regression and classification tasks. The text mentions that \"these networks can handle multiple quantitative responses in a seamless fashion,\" indicating their versatility compared to traditional regression models.\n",
       "\n",
       "3. **High Signal-to-Noise Ratio**: Neural networks are especially effective in problems with a high signal-to-noise ratio, where the goal is prediction without interpretation. The discussion notes that these tools \"have been shown to compete well with the best learning methods on many problems,\" highlighting their performance advantage in certain contexts.\n",
       "\n",
       "4. **Complex Interactions**: In neural networks, each input can enter the model in many places and in a nonlinear fashion, which allows for the modeling of complex interactions between variables. This is contrasted with traditional regression, which may struggle to capture such interactions without explicitly including interaction terms.\n",
       "\n",
       "In summary, while traditional regression methods are useful for simpler, linear relationships and for interpreting the roles of individual inputs, neural networks provide a powerful alternative for modeling complex, nonlinear relationships and for tasks where prediction is prioritized over interpretability.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Ask a question (or type 'exit' to exit):  What programs in the real world use regressions over neural networks?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No answers found in context library. Searching the web...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "**Response:** In the real world, there are several scenarios where traditional regression methods are preferred over neural networks. Here are some examples:\n",
       "\n",
       "1. **Medical Research**: In many medical studies, linear regression is used to interpret the relationship between variables due to its simplicity and ease of interpretation. For instance, a study may examine the effect of dosage on patient recovery using linear regression models, which offer clear insights into the importance of the dosage amounts.\n",
       "\n",
       "2. **Economics**: Economists often use regression analysis to model relationships between economic indicators. For example, multiple linear regression might be employed to assess the impact of education levels and household income on spending patterns. The coefficients provide direct interpretation of how much each predictor influences the outcome variable.\n",
       "\n",
       "3. **Social Sciences**: In fields such as sociology or psychology, regression techniques are utilized to analyze survey data. Researchers might use logistic regression to understand how demographic factors affect voting behavior, as the results are easier to communicate to a broader audience.\n",
       "\n",
       "4. **Finance**: Portfolio management often involves the use of regression techniques to determine the relationship between asset returns and market factors. For instance, the Capital Asset Pricing Model (CAPM) relies on linear regression to estimate the expected return of an asset based on its risk relative to the market.\n",
       "\n",
       "These applications highlight how traditional regression methods still play a crucial role across various fields, largely due to their interpretability and the straightforward nature of the relationships they model.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.schema import messages_from_dict, messages_to_dict\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "from IPython.display import Markdown\n",
    "from IPython.display import Latex\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from openai import OpenAI\n",
    "import numpy as np\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Brave Search API configuration\n",
    "BRAVE_SEARCH_API_KEY = os.environ.get(\"BRAVE_SEARCH_API_KEY\")\n",
    "BRAVE_SEARCH_ENDPOINT = \"https://api.search.brave.com/res/v1/web/search\"\n",
    "\n",
    "# OpenAI API configuration\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "\n",
    "# 1. Web Search Integration using Brave Search\n",
    "def brave_search(query, count=5):\n",
    "    headers = {\n",
    "        \"Accept\": \"application/json\",\n",
    "        \"X-Subscription-Token\": BRAVE_SEARCH_API_KEY\n",
    "    }\n",
    "    params = {\n",
    "        \"q\": query,\n",
    "        \"count\": count\n",
    "    }\n",
    "    response = requests.get(BRAVE_SEARCH_ENDPOINT, headers=headers, params=params)\n",
    "    return response.json()['web']['results']\n",
    "\n",
    "# 2. Web Scraping and Content Extraction\n",
    "def extract_content(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    return soup.get_text(separator=' ', strip=True)\n",
    "\n",
    "# 3. Text Processing (max length for ada is 8192)\n",
    "def preprocess_text(text, max_length=8192):\n",
    "    # Simple preprocessing: truncate to max_length characters\n",
    "    return text[:max_length]\n",
    "\n",
    "# 4. Embedding Generation using OpenAI\n",
    "def generate_embedding(text):\n",
    "    response = client.embeddings.create(\n",
    "        input=text,\n",
    "        model=\"text-embedding-ada-002\"\n",
    "    )\n",
    "    return response.data[0].embedding\n",
    "\n",
    "# 5. Retrieval\n",
    "def retrieve_relevant_info(query_embedding, doc_embeddings, docs, top_k=5):\n",
    "    similarities = np.dot(doc_embeddings, query_embedding)\n",
    "    top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "    return [docs[i] for i in top_indices]\n",
    "\n",
    "# 6 & 7. Prompt Engineering and Language Model Integration\n",
    "def generate_answer(history, query, relevant_info):\n",
    "    prompt = f\"\"\"Given the following information, please provide a concise and informative answer to the query based on the relevant information.\n",
    "\n",
    "History: {history}\n",
    "\n",
    "Query: {query}\n",
    "\n",
    "Relevant Information:\n",
    "{relevant_info}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that provides accurate and concise information based on the given query, chat history, and relevant information retrieved.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]#,\n",
    "        #max_tokens=\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "# Main RAG function\n",
    "def rag_web_search(history, query):\n",
    "    # Perform web search\n",
    "    search_results = brave_search(query)\n",
    "    \n",
    "    # Extract and process content\n",
    "    docs = []\n",
    "    for result in search_results:\n",
    "        content = extract_content(result['url'])\n",
    "        processed_content = preprocess_text(content)\n",
    "        docs.append(processed_content)\n",
    "    \n",
    "    # Generate embeddings\n",
    "    doc_embeddings = [generate_embedding(doc) for doc in docs]\n",
    "    query_embedding = generate_embedding(query)\n",
    "    \n",
    "    # Retrieve relevant information\n",
    "    relevant_info = retrieve_relevant_info(query_embedding, doc_embeddings, docs)\n",
    "    \n",
    "    # Generate final answer\n",
    "    answer = generate_answer(history, query, \"\\n\".join(relevant_info))\n",
    "    \n",
    "    return answer\n",
    "\n",
    "def markdown_to_text(markdown_content):\n",
    "    # Convert Markdown to HTML\n",
    "    html_content = markdown2.markdown(markdown_content)\n",
    "    \n",
    "    # Parse HTML and extract plain text\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "#create a chromadb (vectorized database for RAG) from a given directory of pdf files\n",
    "def create_db_from_pdf_directory(pdf_directory, db_directory):\n",
    "    # initialize an empty list to store all documents\n",
    "    all_documents = []\n",
    "    \n",
    "    # iterate through all files in the directory\n",
    "    for filename in os.listdir(pdf_directory):\n",
    "        if filename.endswith('.pdf'):\n",
    "            pdf_path = os.path.join(pdf_directory, filename)\n",
    "            #print(f\"Processing {filename}...\")\n",
    "            \n",
    "            # load pdf\n",
    "            loader = PyPDFLoader(pdf_path)\n",
    "            documents = loader.load()\n",
    "            \n",
    "            # add documents to the list\n",
    "            all_documents.extend(documents)\n",
    "    \n",
    "    # split text into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    texts = text_splitter.split_documents(all_documents)\n",
    "    \n",
    "    # vector embeddings instance\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    \n",
    "    # create chromadb\n",
    "    db = Chroma.from_documents(texts, embeddings, persist_directory=db_directory)\n",
    "    \n",
    "    print(f\"Database created successfully at {db_directory}\")\n",
    "    return db\n",
    "\n",
    "\n",
    "#specify source and database and create database with it\n",
    "pdf_directory = \"new_pdfs\"\n",
    "\n",
    "db_directory = \"chromadb\"\n",
    "\n",
    "if not os.path.exists(db_directory):\n",
    "    #print(\"Creating new Chroma database...\")\n",
    "    try:\n",
    "        db = create_db_from_pdf_directory(pdf_directory, db_directory)\n",
    "    except ValueError:\n",
    "        print(\"Please add content before chatting. Put one or more pdfs in the new_pdfs dir and run this cell again.\")\n",
    "        sys.exit(1)\n",
    "else:\n",
    "    #print(\"Loading existing Chroma database...\")\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    db = Chroma(embedding_function=embeddings)\n",
    "    #print(\"Database loaded\")\n",
    "\n",
    "\n",
    "def query_db(db, query):\n",
    "    # Create a retriever from the database\n",
    "    retriever = db.as_retriever()\n",
    "    \n",
    "    # Create a ChatOpenAI model\n",
    "    chat = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "    # Add memory to keep track of conversation history\n",
    "    memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "    # Create a Conversational Retrieval Chain with memory\n",
    "    qa_chain = ConversationalRetrievalChain.from_llm(chat, retriever=retriever, memory=memory)\n",
    "    \n",
    "    # Run the query \n",
    "    result = qa_chain.invoke({\"memory\": memory, \"question\": query})\n",
    "    \n",
    "    return result['answer']\n",
    "\n",
    "def digest_new_pdfs():\n",
    "    new_pdf_dir = 'new_pdfs'\n",
    "    used_pdf_dir = 'used_pdfs'\n",
    "\n",
    "    added_documents = []\n",
    "\n",
    "    if os.listdir(new_pdf_dir):\n",
    "\n",
    "        pdf_found = False\n",
    "\n",
    "        for file in os.listdir(new_pdf_dir):\n",
    "            if file.endswith(\".pdf\"):\n",
    "                if not pdf_found:\n",
    "                    pdf_found = true\n",
    "                    print('processing new content')\n",
    "                \n",
    "                \n",
    "                # load pdf\n",
    "                pdf_path = os.path.join(new_pdf_dir, file)\n",
    "                loader = PyPDFLoader(pdf_path)\n",
    "                documents = loader.load()\n",
    "                \n",
    "                # add documents to the list\n",
    "                added_documents.extend(documents)\n",
    "                \n",
    "                source_file = os.path.join(new_pdf_dir, file)\n",
    "                destination_file = os.path.join(used_pdf_dir, file)\n",
    "                shutil.move(source_file, destination_file)\n",
    "\n",
    "        if pdf_found:\n",
    "            # split text into chunks\n",
    "            text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "            texts = text_splitter.split_documents(added_documents)\n",
    "            \n",
    "            # vector embeddings instance\n",
    "            embeddings = OpenAIEmbeddings()\n",
    "        \n",
    "            # add the embeddings to the existing ChromaDB instance\n",
    "            db.add_documents(documents=texts, embeddings=embeddings)\n",
    "    \n",
    "            print('processing complete')\n",
    "\n",
    "def encode_latex(text):\n",
    "    \n",
    "    # Pattern for inline LaTeX: \\(...\\) or \\[...\\]\n",
    "    inline_pattern = r'\\\\[\\(\\[](.+?)\\\\[\\)\\]]'\n",
    "    # Pattern for display LaTeX: \\begin{equation}...\\end{equation} or \\begin{align}...\\end{align}\n",
    "    display_pattern = r'\\\\begin\\{(equation|align)\\}(.+?)\\\\end\\{\\1\\}'\n",
    "    \n",
    "    # Replace inline LaTeX\n",
    "    text = re.sub(inline_pattern, r'$\\1$', text)\n",
    "    \n",
    "    # Replace display LaTeX\n",
    "    text = re.sub(display_pattern, r'$$\\2$$', text, flags=re.DOTALL)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# First digest new pdfs\n",
    "digest_new_pdfs()\n",
    "\n",
    "# Create a retriever from the database\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "# Create a ChatOpenAI model\n",
    "chat = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# Add memory to keep track of conversation history\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# Create a Conversational Retrieval Chain with memory\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(chat, retriever=retriever, memory=memory)\n",
    "\n",
    "history = ''\n",
    "\n",
    "# Interactive loop\n",
    "while True:\n",
    "    query = input(\"\\nAsk a question (or type 'exit' to exit): \")\n",
    "    if query.lower() == 'exit':\n",
    "        break\n",
    "    else:\n",
    "        # Modify and run the query \n",
    "        modified_query = query + \" enclose any latex expressions, symbols, or equations in $ and use textual evidence where helpful\"\n",
    "        result = qa_chain({\"question\": modified_query})\n",
    "        answer = result['answer']\n",
    "\n",
    "        if (answer == \"I don't know.\" or answer.startswith(\"The provided context does not contain\")):\n",
    "            print('No answers found in context library. Searching the web...')\n",
    "            answer = rag_web_search(history, query + \" enclose any latex explressions, symbols, or equations in $ and cite specific source links used \")\n",
    "        \n",
    "        history += f\"query: {query}\\n answer: {answer}\\n\"\n",
    "        display(Markdown('\\n**Response:** ' + encode_latex(answer) + '\\n'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2226801a-e949-4d35-a3ce-965a27177a9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
