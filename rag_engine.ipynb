{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "69a68fe3-9c64-4aff-9ff1-95eb66fac8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Ask a question (or type 'exit' to exit):  What's the best way to get on a consistent sleep schedule?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No answers found in context library. Searching the web: \n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Response:** To get on a consistent sleep schedule, you can follow these strategies:\n",
       "\n",
       "1. **Set a Regular Sleep Schedule**: Go to bed and wake up at the same time every day, even on weekends. This helps regulate your body's internal clock (circadian rhythm).\n",
       "\n",
       "2. **Create a Bedtime Routine**: Develop a relaxing pre-sleep routine that signals your body it's time to wind down. This could include activities such as reading, taking a warm bath, or practicing meditation.\n",
       "\n",
       "3. **Limit Screen Time**: Reduce exposure to screens (phones, computers, TVs) at least 30 minutes before bedtime, as the blue light emitted can interfere with melatonin production.\n",
       "\n",
       "4. **Make Your Sleep Environment Comfortable**: Ensure your bedroom is conducive to sleepâ€”dark, cool, and quiet. Consider using blackout curtains, earplugs, or white noise machines.\n",
       "\n",
       "5. **Watch Your Diet and Caffeine Intake**: Avoid large meals, caffeine, and alcohol close to bedtime, as these can disrupt sleep.\n",
       "\n",
       "6. **Get Regular Exercise**: Engage in physical activity during the day to help you fall asleep faster and enjoy deeper sleep.\n",
       "\n",
       "7. **Seek Natural Light**: Spend time outdoors during the day to help regulate your sleep-wake cycle.\n",
       "\n",
       "By implementing these strategies into your daily routine, you can improve your chances of maintaining a consistent sleep schedule.\n",
       "\n",
       "For more detailed guidance, consider checking resources on sleep hygiene from reputable health organizations such as the National Sleep Foundation.\n",
       "\n",
       "References:\n",
       "- National Sleep Foundation - [Sleep Hygiene](https://www.sleepfoundation.org/sleep-hygiene)\n",
       "- Centers for Disease Control and Prevention - [Sleep and Sleep Disorders](https://www.cdc.gov/sleep/index.html)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Ask a question (or type 'exit' to exit):  exit\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.schema import messages_from_dict, messages_to_dict\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "from IPython.display import Markdown\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from openai import OpenAI\n",
    "import numpy as np\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Brave Search API configuration\n",
    "BRAVE_SEARCH_API_KEY = os.environ.get(\"BRAVE_SEARCH_API_KEY\")\n",
    "BRAVE_SEARCH_ENDPOINT = \"https://api.search.brave.com/res/v1/web/search\"\n",
    "\n",
    "# OpenAI API configuration\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "\n",
    "# 1. Web Search Integration using Brave Search\n",
    "def brave_search(query, count=5):\n",
    "    headers = {\n",
    "        \"Accept\": \"application/json\",\n",
    "        \"X-Subscription-Token\": BRAVE_SEARCH_API_KEY\n",
    "    }\n",
    "    params = {\n",
    "        \"q\": query,\n",
    "        \"count\": count\n",
    "    }\n",
    "    response = requests.get(BRAVE_SEARCH_ENDPOINT, headers=headers, params=params)\n",
    "    return response.json()['web']['results']\n",
    "\n",
    "# 2. Web Scraping and Content Extraction\n",
    "def extract_content(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    return soup.get_text(separator=' ', strip=True)\n",
    "\n",
    "# 3. Text Processing\n",
    "def preprocess_text(text, max_length=10000):\n",
    "    # Simple preprocessing: truncate to max_length characters\n",
    "    return text[:max_length]\n",
    "\n",
    "# 4. Embedding Generation using OpenAI\n",
    "def generate_embedding(text):\n",
    "    response = client.embeddings.create(\n",
    "        input=text,\n",
    "        model=\"text-embedding-ada-002\"\n",
    "    )\n",
    "    return response.data[0].embedding\n",
    "\n",
    "# 5. Retrieval\n",
    "def retrieve_relevant_info(query_embedding, doc_embeddings, docs, top_k=3):\n",
    "    similarities = np.dot(doc_embeddings, query_embedding)\n",
    "    top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "    return [docs[i] for i in top_indices]\n",
    "\n",
    "# 6 & 7. Prompt Engineering and Language Model Integration\n",
    "def generate_answer(history, query, relevant_info):\n",
    "    prompt = f\"\"\"Given the following information, please provide a concise and informative answer to the query.\n",
    "\n",
    "History: {history}\n",
    "\n",
    "Query: {query}\n",
    "\n",
    "Relevant Information:\n",
    "{relevant_info}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that provides accurate and concise information based on the given query, chat history, and relevant information retrieved.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]#,\n",
    "        #max_tokens=\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "# Main RAG function\n",
    "def rag_web_search(history, query):\n",
    "    # Perform web search\n",
    "    search_results = brave_search(query)\n",
    "    \n",
    "    # Extract and process content\n",
    "    docs = []\n",
    "    for result in search_results:\n",
    "        content = extract_content(result['url'])\n",
    "        processed_content = preprocess_text(content)\n",
    "        docs.append(processed_content)\n",
    "    \n",
    "    # Generate embeddings\n",
    "    doc_embeddings = [generate_embedding(doc) for doc in docs]\n",
    "    query_embedding = generate_embedding(query)\n",
    "    \n",
    "    # Retrieve relevant information\n",
    "    relevant_info = retrieve_relevant_info(query_embedding, doc_embeddings, docs)\n",
    "    \n",
    "    # Generate final answer\n",
    "    answer = generate_answer(history, query, \"\\n\".join(relevant_info))\n",
    "    \n",
    "    return answer\n",
    "\n",
    "def markdown_to_text(markdown_content):\n",
    "    # Convert Markdown to HTML\n",
    "    html_content = markdown2.markdown(markdown_content)\n",
    "    \n",
    "    # Parse HTML and extract plain text\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "#create a chromadb (vectorized database for RAG) from a given directory of pdf files\n",
    "def create_db_from_pdf_directory(pdf_directory, db_directory):\n",
    "    # initialize an empty list to store all documents\n",
    "    all_documents = []\n",
    "    \n",
    "    # iterate through all files in the directory\n",
    "    for filename in os.listdir(pdf_directory):\n",
    "        if filename.endswith('.pdf'):\n",
    "            pdf_path = os.path.join(pdf_directory, filename)\n",
    "            #print(f\"Processing {filename}...\")\n",
    "            \n",
    "            # load pdf\n",
    "            loader = PyPDFLoader(pdf_path)\n",
    "            documents = loader.load()\n",
    "            \n",
    "            # add documents to the list\n",
    "            all_documents.extend(documents)\n",
    "    \n",
    "    # split text into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    texts = text_splitter.split_documents(all_documents)\n",
    "    \n",
    "    # vector embeddings instance\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    \n",
    "    # create chromadb\n",
    "    db = Chroma.from_documents(texts, embeddings, persist_directory=db_directory)\n",
    "    \n",
    "    print(f\"Database created successfully at {db_directory}\")\n",
    "    return db\n",
    "\n",
    "\n",
    "#specify source and database and create database with it\n",
    "pdf_directory = \"pdfsnew_\"\n",
    "\n",
    "db_directory = \"chromadb\"\n",
    "\n",
    "if not os.path.exists(db_directory):\n",
    "    #print(\"Creating new Chroma database...\")\n",
    "    db = create_db_from_pdf_directory(pdf_directory, db_directory)\n",
    "else:\n",
    "    #print(\"Loading existing Chroma database...\")\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    db = Chroma(embedding_function=embeddings)\n",
    "    #print(\"Database loaded\")\n",
    "\n",
    "\n",
    "def query_db(db, query):\n",
    "    # Create a retriever from the database\n",
    "    retriever = db.as_retriever()\n",
    "    \n",
    "    # Create a ChatOpenAI model\n",
    "    chat = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "    # Add memory to keep track of conversation history\n",
    "    memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "    # Create a Conversational Retrieval Chain with memory\n",
    "    qa_chain = ConversationalRetrievalChain.from_llm(chat, retriever=retriever, memory=memory)\n",
    "    \n",
    "    # Run the query \n",
    "    result = qa_chain({\"memory\": memory, \"question\": query})\n",
    "    \n",
    "    return result['answer']\n",
    "\n",
    "def digest_new_pdfs():\n",
    "    new_pdf_dir = 'new_pdfs'\n",
    "    used_pdf_dir = 'used_pdfs'\n",
    "\n",
    "    added_documents = []\n",
    "\n",
    "    if os.listdir(new_pdf_dir):\n",
    "\n",
    "        print('processing new content')\n",
    "        for file in os.listdir(new_pdf_dir):\n",
    "            if file.endswith(\".pdf\"):\n",
    "                # load pdf\n",
    "                pdf_path = os.path.join(new_pdf_dir, file)\n",
    "                loader = PyPDFLoader(pdf_path)\n",
    "                documents = loader.load()\n",
    "                \n",
    "                # add documents to the list\n",
    "                added_documents.extend(documents)\n",
    "                \n",
    "                source_file = os.path.join(new_pdf_dir, file)\n",
    "                destination_file = os.path.join(used_pdf_dir, file)\n",
    "                shutil.move(source_file, destination_file)\n",
    "    \n",
    "        # split text into chunks\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "        texts = text_splitter.split_documents(added_documents)\n",
    "        \n",
    "        # vector embeddings instance\n",
    "        embeddings = OpenAIEmbeddings()\n",
    "    \n",
    "        # add the embeddings to the existing ChromaDB instance\n",
    "        db.add_documents(documents=texts, embeddings=embeddings)\n",
    "\n",
    "        print('processing complete')\n",
    "\n",
    "# First digest new pdfs\n",
    "digest_new_pdfs()\n",
    "\n",
    "# Create a retriever from the database\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "# Create a ChatOpenAI model\n",
    "chat = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# Add memory to keep track of conversation history\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# Create a Conversational Retrieval Chain with memory\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(chat, retriever=retriever, memory=memory)\n",
    "\n",
    "history = ''\n",
    "\n",
    "# Interactive loop\n",
    "while True:\n",
    "    query = input(\"\\nAsk a question (or type 'exit' to exit): \")\n",
    "    if query.lower() == 'exit':\n",
    "        break\n",
    "    elif query.lower() == 'add':\n",
    "        # Add new PDFs to ChromaDB\n",
    "        new_pdf_directory = input(\"Enter the path to the directory with new PDFs: \")\n",
    "        # Function to add new PDFs would go here\n",
    "        # add_new_pdfs_to_chromadb(new_pdf_directory, db)\n",
    "        print(\"New PDFs added to ChromaDB.\")\n",
    "    else:\n",
    "        #response = query_db(db, query)\n",
    "        \n",
    "        # Run the query \n",
    "        modified_query = query + \" use textual evidence where beneficial and put dollar signs on each side of any equations used\"\n",
    "        result = qa_chain({\"question\": modified_query})\n",
    "        answer = result['answer']\n",
    "        \n",
    "        if (answer == \"I don't know.\"):\n",
    "            print('No answers found in context library. Searching the web: ')\n",
    "            answer = rag_web_search(history, query + \" cite specific source links used and put dollar signs on each side of any latex expressions or equations used\")\n",
    "            \n",
    "        history += f\"query: {query}\\n answer: {answer}\\n\"\n",
    "        display(Markdown(\"**Response:** \" + answer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2d613020-29bb-4273-82e0-7324c9b52f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Ask a question (or type 'exit' to exit):  What is statistical decision theory?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Response:** Statistical decision theory provides a framework for developing models that predict outcomes based on input data. It involves the use of random variables and probability spaces to understand the relationship between inputs and outputs. Specifically, it considers a real-valued random input vector $X \\in \\mathbb{R}^p$ and a real-valued random output variable $Y \\in \\mathbb{R}$, with a joint distribution denoted as $Pr(X, Y)$. The goal is to find a function $f(X)$ that predicts $Y$ given values of the input $X$.\n",
       "\n",
       "In this context, a loss function $L(Y, f(X))$ is required to penalize errors in prediction. This loss function is crucial for evaluating the performance of the predictive model. The theory emphasizes the importance of understanding the statistical properties of the data and the implications of different decision-making strategies based on the predictions made by the model."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Ask a question (or type 'exit' to exit):  What are some real-world applications of it?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Response:** Some real-world applications of statistical decision theory include:\n",
       "\n",
       "1. **Medical Predictions**: For instance, predicting whether a patient, hospitalized due to a heart attack, will have a second heart attack based on demographic, diet, and clinical measurements. This involves using a function $f(X)$ to predict the output variable $Y$, which in this case is the likelihood of a second heart attack given the input vector $X$.\n",
       "\n",
       "2. **Financial Forecasting**: Predicting the price of a stock in 6 months based on company performance measures and economic data. Here, the joint distribution $Pr(X,Y)$ is used to understand the relationship between the input variables (company performance and economic data) and the output variable (stock price).\n",
       "\n",
       "3. **Image Recognition**: Identifying numbers in a handwritten ZIP code from a digitized image. This application utilizes statistical decision theory to classify the input data (the image) into the correct output (the numbers).\n",
       "\n",
       "4. **Health Monitoring**: Estimating the amount of glucose in the blood of a diabetic person from the infrared absorption spectrum of that personâ€™s blood. This involves modeling the relationship between the input (infrared absorption spectrum) and the output (glucose level) using statistical decision theory.\n",
       "\n",
       "5. **Risk Assessment**: Identifying risk factors for prostate cancer based on clinical and demographic variables. This application uses the framework of statistical decision theory to analyze the input variables and predict the risk of developing prostate cancer.\n",
       "\n",
       "These applications illustrate how statistical decision theory provides a framework for developing predictive models across various fields, including medicine, finance, and image processing."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Ask a question (or type 'exit' to exit):  exit\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563bfb03-b06a-4fb2-b11d-7954e8438f7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
